{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "skl_pca.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toruuno/report_ml/blob/master/skl_pca.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzRg2mf8MPV3"
      },
      "source": [
        "#https://ohke.hateblo.jp/entry/2017/08/11/230000を参考に利用しています。"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0v6XSaKJTyK"
      },
      "source": [
        "## Googleドライブのマウント"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLQ_EkmX42V5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bcf0df1b-f6bc-49c0-aa8a-acf36a715a12"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIW-96d6MPV5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTCEbaLSJaQl"
      },
      "source": [
        "## sys.pathの設定"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kvrOJAbJfA_"
      },
      "source": [
        "以下では，Googleドライブのマイドライブ直下にstudy_ai_mlフォルダを置くことを仮定しています．必要に応じて，パスを変更してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuYtUsBrMPV7"
      },
      "source": [
        "cancer_df = pd.read_csv('/content/drive/My Drive/pca/cancer.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIViV_TQMPV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939d99db-0c78-40a2-ccfd-de4c29c5f2b8"
      },
      "source": [
        "print('cancer df shape: {}'.format(cancer_df.shape))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cancer df shape: (569, 33)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp8LS3QtMPV_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "72b9c4b2-f060-4d15-e63b-2dae24bab937"
      },
      "source": [
        "cancer_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "      <th>Unnamed: 32</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 33 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  ...  fractal_dimension_worst  Unnamed: 32\n",
              "0      842302         M  ...                  0.11890          NaN\n",
              "1      842517         M  ...                  0.08902          NaN\n",
              "2    84300903         M  ...                  0.08758          NaN\n",
              "3    84348301         M  ...                  0.17300          NaN\n",
              "4    84358402         M  ...                  0.07678          NaN\n",
              "..        ...       ...  ...                      ...          ...\n",
              "564    926424         M  ...                  0.07115          NaN\n",
              "565    926682         M  ...                  0.06637          NaN\n",
              "566    926954         M  ...                  0.07820          NaN\n",
              "567    927241         M  ...                  0.12400          NaN\n",
              "568     92751         B  ...                  0.07039          NaN\n",
              "\n",
              "[569 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvHpdBL2MPWB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "ab09d137-327b-4b22-e1c6-72a6d0b424e0"
      },
      "source": [
        "cancer_df.drop('Unnamed: 32', axis=1, inplace=True)\n",
        "cancer_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>diagnosis</th>\n",
              "      <th>radius_mean</th>\n",
              "      <th>texture_mean</th>\n",
              "      <th>perimeter_mean</th>\n",
              "      <th>area_mean</th>\n",
              "      <th>smoothness_mean</th>\n",
              "      <th>compactness_mean</th>\n",
              "      <th>concavity_mean</th>\n",
              "      <th>concave points_mean</th>\n",
              "      <th>symmetry_mean</th>\n",
              "      <th>fractal_dimension_mean</th>\n",
              "      <th>radius_se</th>\n",
              "      <th>texture_se</th>\n",
              "      <th>perimeter_se</th>\n",
              "      <th>area_se</th>\n",
              "      <th>smoothness_se</th>\n",
              "      <th>compactness_se</th>\n",
              "      <th>concavity_se</th>\n",
              "      <th>concave points_se</th>\n",
              "      <th>symmetry_se</th>\n",
              "      <th>fractal_dimension_se</th>\n",
              "      <th>radius_worst</th>\n",
              "      <th>texture_worst</th>\n",
              "      <th>perimeter_worst</th>\n",
              "      <th>area_worst</th>\n",
              "      <th>smoothness_worst</th>\n",
              "      <th>compactness_worst</th>\n",
              "      <th>concavity_worst</th>\n",
              "      <th>concave points_worst</th>\n",
              "      <th>symmetry_worst</th>\n",
              "      <th>fractal_dimension_worst</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842302</td>\n",
              "      <td>M</td>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>926424</td>\n",
              "      <td>M</td>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>926682</td>\n",
              "      <td>M</td>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>926954</td>\n",
              "      <td>M</td>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>927241</td>\n",
              "      <td>M</td>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>92751</td>\n",
              "      <td>B</td>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           id diagnosis  ...  symmetry_worst  fractal_dimension_worst\n",
              "0      842302         M  ...          0.4601                  0.11890\n",
              "1      842517         M  ...          0.2750                  0.08902\n",
              "2    84300903         M  ...          0.3613                  0.08758\n",
              "3    84348301         M  ...          0.6638                  0.17300\n",
              "4    84358402         M  ...          0.2364                  0.07678\n",
              "..        ...       ...  ...             ...                      ...\n",
              "564    926424         M  ...          0.2060                  0.07115\n",
              "565    926682         M  ...          0.2572                  0.06637\n",
              "566    926954         M  ...          0.2218                  0.07820\n",
              "567    927241         M  ...          0.4087                  0.12400\n",
              "568     92751         B  ...          0.2871                  0.07039\n",
              "\n",
              "[569 rows x 32 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_reBSjCMPWC"
      },
      "source": [
        "・diagnosis: 診断結果 (良性がB / 悪性がM)\n",
        "・説明変数は3列以降、目的変数を2列目としロジスティック回帰で分類"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFSzNUEkMPWD"
      },
      "source": [
        "# 目的変数の抽出\n",
        "y = cancer_df.diagnosis.apply(lambda d: 1 if d == 'M' else 0)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kntW_wxGMPWE"
      },
      "source": [
        "# 説明変数の抽出\n",
        "X = cancer_df.loc[:, 'radius_mean':]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBJNm3jbMPWG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "277d542e-97ed-421e-b6f8-9ed6ee72895d"
      },
      "source": [
        "# 学習用とテスト用でデータを分離\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# 標準化\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# ロジスティック回帰で学習\n",
        "logistic = LogisticRegressionCV(cv=10, random_state=0)\n",
        "logistic.fit(X_train_scaled, y_train)\n",
        "\n",
        "# 検証\n",
        "print('Train score: {:.3f}'.format(logistic.score(X_train_scaled, y_train)))\n",
        "print('Test score: {:.3f}'.format(logistic.score(X_test_scaled, y_test)))\n",
        "print('Confustion matrix:\\n{}'.format(confusion_matrix(y_true=y_test, y_pred=logistic.predict(X_test_scaled))))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train score: 0.988\n",
            "Test score: 0.972\n",
            "Confustion matrix:\n",
            "[[89  1]\n",
            " [ 3 50]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o0q_FfhMPWH"
      },
      "source": [
        "・検証スコア97%で分類できることを確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-kwsEXQMPWI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "711cfcdd-0303-478f-8ee0-9baf94adee32"
      },
      "source": [
        "pca = PCA(n_components=30)\n",
        "pca.fit(X_train_scaled)\n",
        "plt.bar([n for n in range(1, len(pca.explained_variance_ratio_)+1)], pca.explained_variance_ratio_)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 30 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsmvUmuUMPWJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "e647e7ed-57e5-47ee-ccab-6b5b70cf55c4"
      },
      "source": [
        "# PCA\n",
        "# 次元数2まで圧縮\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "print('X_train_pca shape: {}'.format(X_train_pca.shape))\n",
        "# X_train_pca shape: (426, 2)\n",
        "\n",
        "# 寄与率\n",
        "print('explained variance ratio: {}'.format(pca.explained_variance_ratio_))\n",
        "# explained variance ratio: [ 0.43315126  0.19586506]\n",
        "\n",
        "# 散布図にプロット\n",
        "temp = pd.DataFrame(X_train_pca)\n",
        "temp['Outcome'] = y_train.values\n",
        "b = temp[temp['Outcome'] == 0]\n",
        "m = temp[temp['Outcome'] == 1]\n",
        "plt.scatter(x=b[0], y=b[1], marker='o') # 良性は○でマーク\n",
        "plt.scatter(x=m[0], y=m[1], marker='^') # 悪性は△でマーク\n",
        "plt.xlabel('PC 1') # 第1主成分をx軸\n",
        "plt.ylabel('PC 2') # 第2主成分をy軸"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train_pca shape: (426, 2)\n",
            "explained variance ratio: [0.43315126 0.19586506]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'PC 2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBOSr0SFx4_r"
      },
      "source": [
        "### とりあえず自分なりに実習をする(PCAで圧縮したものがモデルの精度にどう影響するか体感)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjQxTSyqMPWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "836cf417-68ce-4b36-c7c1-868c472fff8a"
      },
      "source": [
        "#N次元に圧縮したうえでロジスティック回帰して精度を見てみる\n",
        "#説明変数全体をStandardScaleしてから主成分分析でN次元に圧縮して\n",
        "#そこから訓練データとテストデータに分けて検証すれば良いかな?\n",
        "\n",
        "# 標準化\n",
        "scaler_pcaN = StandardScaler()\n",
        "X_scaled = scaler_pcaN.fit_transform(X)\n",
        "\n",
        "#主成分分析(N次元)=>ロジスティック回帰\n",
        "#Nは\"TOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\"しない程度に設定\n",
        "#精度を追求しないならば「5次元程度でも十分戦えそう」\n",
        "#精度を追求したとしても「13次元で十分かな？」\n",
        "#という「感想」に至った\n",
        "for n in range(1,14,1):\n",
        "  #PCA(N次元)\n",
        "  print('-----PCA{}-----'.format(n))\n",
        "  pcaN = PCA(n_components=n)\n",
        "  X_pcaN = pcaN.fit_transform(X_scaled)\n",
        "  print(X_pcaN.shape)\n",
        "\n",
        "  # 学習用とテスト用でデータを分離\n",
        "  X_train_pcaN, X_test_pcaN, y_train_pcaN, y_test_pcaN = train_test_split(X_pcaN, y, random_state=0)\n",
        "\n",
        "  # ロジスティック回帰で学習\n",
        "  logistic_pcaN = LogisticRegressionCV(cv=10, random_state=0)\n",
        "  logistic_pcaN.fit(X_train_pcaN, y_train_pcaN)\n",
        "\n",
        "  # 検証\n",
        "  print('Train score: {:.3f}'.format(logistic_pcaN.score(X_train_pcaN, y_train_pcaN)))\n",
        "  print('Test score: {:.3f}'.format(logistic_pcaN.score(X_test_pcaN, y_test_pcaN)))\n",
        "  print('Confustion matrix:\\n{}'.format(confusion_matrix(y_true=y_test_pcaN, y_pred=logistic_pcaN.predict(X_test_pcaN))))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----PCA1-----\n",
            "(569, 1)\n",
            "Train score: 0.923\n",
            "Test score: 0.902\n",
            "Confustion matrix:\n",
            "[[82  8]\n",
            " [ 6 47]]\n",
            "-----PCA2-----\n",
            "(569, 2)\n",
            "Train score: 0.960\n",
            "Test score: 0.951\n",
            "Confustion matrix:\n",
            "[[84  6]\n",
            " [ 1 52]]\n",
            "-----PCA3-----\n",
            "(569, 3)\n",
            "Train score: 0.955\n",
            "Test score: 0.937\n",
            "Confustion matrix:\n",
            "[[84  6]\n",
            " [ 3 50]]\n",
            "-----PCA4-----\n",
            "(569, 4)\n",
            "Train score: 0.969\n",
            "Test score: 0.972\n",
            "Confustion matrix:\n",
            "[[87  3]\n",
            " [ 1 52]]\n",
            "-----PCA5-----\n",
            "(569, 5)\n",
            "Train score: 0.984\n",
            "Test score: 0.965\n",
            "Confustion matrix:\n",
            "[[88  2]\n",
            " [ 3 50]]\n",
            "-----PCA6-----\n",
            "(569, 6)\n",
            "Train score: 0.984\n",
            "Test score: 0.965\n",
            "Confustion matrix:\n",
            "[[88  2]\n",
            " [ 3 50]]\n",
            "-----PCA7-----\n",
            "(569, 7)\n",
            "Train score: 0.974\n",
            "Test score: 0.958\n",
            "Confustion matrix:\n",
            "[[89  1]\n",
            " [ 5 48]]\n",
            "-----PCA8-----\n",
            "(569, 8)\n",
            "Train score: 0.984\n",
            "Test score: 0.951\n",
            "Confustion matrix:\n",
            "[[88  2]\n",
            " [ 5 48]]\n",
            "-----PCA9-----\n",
            "(569, 9)\n",
            "Train score: 0.986\n",
            "Test score: 0.958\n",
            "Confustion matrix:\n",
            "[[87  3]\n",
            " [ 3 50]]\n",
            "-----PCA10-----\n",
            "(569, 10)\n",
            "Train score: 0.986\n",
            "Test score: 0.958\n",
            "Confustion matrix:\n",
            "[[87  3]\n",
            " [ 3 50]]\n",
            "-----PCA11-----\n",
            "(569, 11)\n",
            "Train score: 0.986\n",
            "Test score: 0.965\n",
            "Confustion matrix:\n",
            "[[88  2]\n",
            " [ 3 50]]\n",
            "-----PCA12-----\n",
            "(569, 12)\n",
            "Train score: 0.986\n",
            "Test score: 0.965\n",
            "Confustion matrix:\n",
            "[[88  2]\n",
            " [ 3 50]]\n",
            "-----PCA13-----\n",
            "(569, 13)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train score: 0.986\n",
            "Test score: 0.972\n",
            "Confustion matrix:\n",
            "[[88  2]\n",
            " [ 2 51]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}